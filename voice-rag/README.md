# ğŸ™ï¸ Voice-RAG: Chat with Audio Recordings

This project enables conversational question-answering over audio content using Retrieval-Augmented Generation (RAG). It transcribes speech using Whisper, indexes it with semantic embeddings, and uses Ollama-powered LLMs to generate intelligent answers to your queries.

---

## ğŸš€ Key Features

- ğŸ§ Upload and transcribe audio files (MP3/WAV)
- ğŸ§  Convert spoken words into searchable context using OpenAI Whisper
- ğŸ” Semantic retrieval with Ollama embeddings
- ğŸ’¬ Natural language Q&A with DeepSeek LLM
- ğŸ§¹ Cleans output for smoother UX

---

## ğŸ› ï¸ Tech Stack

- Python
- Streamlit (Web UI)
- Whisper (Audio transcription)
- LangChain (RAG pipeline, chunking, prompting)
- Ollama (DeepSeek for embeddings and LLM)
- InMemoryVectorStore (Document search engine)

---

## ğŸ“ Project Structure

```
voice_rag/
â”œâ”€â”€ voice_rag.py             # Main Streamlit app
â”œâ”€â”€ audios/                  # Uploaded audio files
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md                # Project documentation
```

---

## ğŸ”„ How It Works

1. Upload an MP3 or WAV audio file via the UI
2. Whisper transcribes the audio to text
3. Transcribed text is chunked and embedded using DeepSeek
4. A user asks a question in natural language
5. Relevant audio segments are retrieved semantically
6. DeepSeek LLM generates an answer based on the retrieved context

---

## ğŸ“¦ Installation

Clone the repo:

```bash
git clone https://github.com/yourusername/ollama-project.git
cd ollama-project/voice_rag
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Ensure Whisper and DeepSeek models are downloaded:

```bash
# Whisper model will be downloaded on first use
# Ollama models must be pulled manually
ollama run deepseek-r1:8b
```

You may also need ffmpeg for Whisper:

```bash
# For Linux/macOS
brew install ffmpeg
# For Windows: download and add ffmpeg to PATH
```

---

## â–¶ï¸ Run the App

```bash
streamlit run voice_rag.py
```

Upload your audio file and start asking questions!

---

## ğŸ§ª Example Use Cases

- Analyze podcast recordings
- Summarize or query meeting audio
- Extract insights from lectures or interviews
- Language learning: quiz yourself on audio clips

---

## ğŸ§  Model Pipeline

- ğŸ¤ Audio Transcription: Whisper (medium.en)
- ğŸ”¤ Text Embedding: Ollama Embeddings (DeepSeek-r1:8b)
- ğŸ§  RAG LLM: DeepSeek-r1:8b via Ollama
- ğŸ” Retrieval: InMemoryVectorStore
- ğŸ§© Text Chunking: RecursiveCharacterTextSplitter

---

## ğŸ“Œ Notes

- Transcription and vector indexing are stored in memory only (no database).
- All audio files are saved in the audios/ directory.
- Cleans out special <think> tags from LLM outputs.

---

## ğŸ¤ Contributing

Found a bug or have an idea for improvement? Pull requests and issues are welcome!

---

## ğŸ“§ Contact

ğŸ“¨ Email: shaishavsurati06@email.com  
ğŸ”— LinkedIn: [Shaishav Surati ğŸ‡®ğŸ‡³](https://linkedin.com/in/shaishavsurati)

---

Let your voice be the context â€” literally. ğŸ¤ğŸ’¡