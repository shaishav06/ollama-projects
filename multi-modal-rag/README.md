# ğŸ§  Multi-Modal RAG: Chat with PDFs (Text + Images)

This project enables interactive Q&A over PDF documents with both text and visual content (images, tables) using Multi-Modal Retrieval-Augmented Generation (RAG). Powered by Ollama's large language models, it combines OCR, visual understanding, and semantic search for an enhanced AI assistant experience.

---

## ğŸ”¥ Key Features

- ğŸ“„ Upload and parse PDF documents
- âœ‚ï¸ Text chunking for better retrieval
- ğŸ§  Semantic search using Ollama embeddings (In-Memory VectorStore)
- ğŸ–¼ï¸ High-resolution visual block extraction (Images + Tables)
- ğŸ” Visual reasoning using multi-modal LLM (Gemma3:27b)
- ğŸ’¬ Natural language Q&A powered by Ollama
- ğŸ“š Full multi-modal context for high-quality answers

---

## ğŸ›  Tech Stack

- Python
- Streamlit (UI)
- LangChain (LLMs, chains, prompts, retrievers)
- Unstructured (PDF parsing and image partitioning)
- Ollama (LLMs and Embeddings)
- Gemma3:27b (for multi-modal reasoning)
- LLaMA3.2 (for embeddings)
- InMemoryVectorStore (retrieval engine)

---

## ğŸ“‚ Project Structure

```
multi_modal_rag/
â”œâ”€â”€ multi_modal_rag.py       # Main Streamlit app
â”œâ”€â”€ pdfs/                    # Uploaded PDFs
â”œâ”€â”€ figures/                 # Extracted images/tables
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md                # Project documentation
```

---

## ğŸš€ How It Works

1. ğŸ“ Upload a PDF through the web UI
2. ğŸ§  Unstructured parses both text and image blocks
3. ğŸ–¼ï¸ Images are analyzed via Gemma3:27b for descriptive captions
4. ğŸ§© Text + image insights are chunked and embedded
5. ğŸ” User question is compared semantically to context
6. ğŸ—£ï¸ Answer is generated using relevant multi-modal knowledge

---

## ğŸ“¦ Installation

Clone the repository:

```bash
git clone https://github.com/yourusername/ollama-project.git
cd ollama-project/multi_modal_rag
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Download NLTK tokenizer if needed:

```bash
python -m nltk.downloader punkt
```

Ensure Ollama is running the necessary models:

```bash
ollama run llama3.2
ollama run gemma3:27b
```

---

## â–¶ï¸ Launch the App

Start the Streamlit application:

```bash
streamlit run multi_modal_rag.py
```

- Upload a PDF
- Ask any question related to the content
- Get concise answers combining text and image intelligence

---

## ğŸ§ª Example Use Cases

- Analyze scientific papers with diagrams and charts
- Summarize reports with both text and data tables
- Extract visual insights from manuals and product guides
- Interactive chat with image-heavy whitepapers

---

## ğŸ§  Model Pipeline

- ğŸ”¤ Text Embedding: llama3.2
- ğŸ“¸ Image Reasoning: gemma3:27b with image binding
- ğŸ—‚ï¸ Context Storage: InMemoryVectorStore
- ğŸ” Text Chunking: RecursiveCharacterTextSplitter
- ğŸ§  RAG Answering: Custom LangChain Prompt + Ollama LLM

---

## ğŸ“Œ Notes

- Images/tables are stored in the figures/ directory.
- This is an in-memory implementation. On restart, context is lost.
- Tested with single PDFs at a time for simplicity.

---

## ğŸ¤ Contributing

Got an idea or found a bug? Feel free to open an issue or PR. Letâ€™s build more intelligent agents together!

---

## ğŸ“§ Contact

ğŸ“¨ Email: shaishavsurati06@email.com  
ğŸ”— LinkedIn: [Shaishav Surati ğŸ‡®ğŸ‡³](https://linkedin.com/in/shaishavsurati)

Letâ€™s build smarter AI tools â€” together! ğŸš€